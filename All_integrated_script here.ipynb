{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e098312c",
   "metadata": {},
   "source": [
    "# Version2.0 and Concise price summary code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "25328938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\web.dev-1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "import time\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from datetime import datetime, timedelta                          \n",
    "from urllib.parse import urlparse, urljoin\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from summarizer import Summarizer\n",
    "import openai\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "blacklist=['vk.com',\n",
    " 'nielsen india',\n",
    " 'redseer consulting',\n",
    " 'livejournal.com',\n",
    " 'arcluster',\n",
    " 'ipsos india',\n",
    " 'indiamart intermesh ltd.',\n",
    " 'mordor intelligence',\n",
    " 'weibo.com',\n",
    " 'youtube.com',\n",
    " '6wresearch',\n",
    " 'answers.yahoo.com',\n",
    " 'ghost.org',\n",
    " 'snapchat.com',\n",
    " 'instagram.com',\n",
    " 'wordpress.com',\n",
    " 'hubpages.com',\n",
    " 'quora.com',\n",
    " 'imarc group',\n",
    " 'vynz research',\n",
    " 'reportsnreports',\n",
    " 'medium.com',\n",
    " 'nielsen bookscan',\n",
    " 'blogspot.com',\n",
    " 'the smart cube',\n",
    " 'smart research insights',\n",
    " 'weebly.com',\n",
    " 'tistory.com',\n",
    " 'ebay.com',\n",
    " 'twitter.com',\n",
    " 'wix.com',\n",
    " 'pinterest.co.uk',\n",
    " 'etsy.com',\n",
    " 'imrb international (kantar millward brown)',\n",
    " 'allied market research',\n",
    " 'ducker worldwide india',\n",
    " 'acumen research and consulting',\n",
    " 'indian market research bureau (imrb)',\n",
    " 'telegram.org',\n",
    " 'stackoverflow.com',\n",
    " 'feedback consulting',\n",
    " 'blogger.com',\n",
    " 'transparency market research (tmr)',\n",
    " 'ask.fm',\n",
    " 'linkedin.com',\n",
    " 'squarespace.com',\n",
    " 'bis research',\n",
    " 'tumblr.com',\n",
    " 'tns india (kantar tns)',\n",
    " 'typepad.com',\n",
    " 'stackexchange.com',\n",
    " 'technavio',\n",
    " 'ken research',\n",
    " 'pinterest.com',\n",
    " 'facebook.com',\n",
    " 'techsci research',\n",
    " 'frost & sullivan india',\n",
    " 'market research india (mrsi)',\n",
    " 'valuenotes',\n",
    " 'adroit market research',\n",
    " 'cygnus research',\n",
    " 'euromonitor international',\n",
    " 'research on india',\n",
    " 'india ratings and research',\n",
    " 'dailymotion.com',\n",
    " 'jimdo.com',\n",
    " 'fortune business insights',\n",
    " 'market data forecast',\n",
    " 'mrss india (majestic market research support services ltd)',\n",
    " 'p&s intelligence (formerly persistence market research)',\n",
    " 'research and markets',\n",
    " 'reddit.com',\n",
    " 'amazon.com',\n",
    " 'marketsandmarkets',\n",
    " 'netscribes','benzinga','consultancy','maximizemarketresearch','openpr','wikipedia','chemanalyst','seedance','woodmac','blog','seedance']\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "keywords = [\"market trends\", \"demand\", \"supply\",'growth','import','export','amonia','price','feedstock','base materials']\n",
    "class MarketAnalysisGenerator:\n",
    "    \n",
    "    def __init__(self, product, country, openai_api_key):\n",
    "        self.product = product\n",
    "        self.country = country\n",
    "        self.openai_api_key = openai_api_key\n",
    "        openai.api_key = self.openai_api_key\n",
    "\n",
    "    @staticmethod\n",
    "    def past_dates(days):\n",
    "        current_date = datetime.now()\n",
    "        past_date = current_date - timedelta(days=days) \n",
    "        return current_date.strftime(\"%m/%d/%Y\"), past_date.strftime(\"%m/%d/%Y\")\n",
    "\n",
    "    def web_scraping(self, days=7):\n",
    "        current_date, past_seven = self.past_dates(days)\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        text = f'{self.product} market situation {self.country}'\n",
    "        start_date = past_seven\n",
    "        end_date = current_date\n",
    "        date_range = f\"&tbs=cdr:1,cd_min:{urllib.parse.quote_plus(start_date)},cd_max:{urllib.parse.quote_plus(end_date)}\"\n",
    "        url = f'https://google.com/search?q={urllib.parse.quote_plus(text)}{date_range}'\n",
    "        \n",
    "        time.sleep(5)\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        search_results = soup.select('div.tF2Cxc')\n",
    "        data = []\n",
    "        fetched_urls_count = 0\n",
    "        for result in search_results:\n",
    "            title = result.select_one('h3').text if result.select_one('h3') else None\n",
    "            link = result.select_one('a')['href'] if result.select_one('a') else None\n",
    "            source_name = result.select_one('div.TbwUpd.NJjxre').text if result.select_one('div.TbwUpd.NJjxre') else None\n",
    "            if not any(blacklisted_item in link for blacklisted_item in blacklist):\n",
    "                try:\n",
    "                    \n",
    "                    source_response = requests.get(link, headers=headers)\n",
    "                    source_soup = BeautifulSoup(source_response.content, 'html.parser')\n",
    "                    source_text = source_soup.get_text()\n",
    "                    doc = nlp(source_text)\n",
    "                    relevant_answers = [sentence.text for sentence in doc.sents if text.lower() in sentence.text.lower()]\n",
    "                    data.append({\n",
    "                        \"Title\": title,\n",
    "                        \"URL\": link,\n",
    "                        \"Source Name\": source_name,\n",
    "                        \"Answer\": \"\\n\".join(relevant_answers)\n",
    "                    })\n",
    "                    fetched_urls_count += 1\n",
    "                    if fetched_urls_count >= 10:\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error fetching content from {link}: {str(e)}\")\n",
    "        df = pd.DataFrame(data, columns=[\"Title\", \"URL\", \"Source Name\", \"Answer\"])\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_domain_from_url(url):\n",
    "        parsed_uri = urlparse(url)\n",
    "        domain = '{uri.netloc}'.format(uri=parsed_uri).replace(\"www.\", \"\")\n",
    "        return domain\n",
    "\n",
    "    @staticmethod\n",
    "    def is_blacklisted(content, blacklist):\n",
    "        return any(keyword in content.lower() for keyword in blacklist)\n",
    "\n",
    "    def extract_content_from_url(self, url, headers):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            links = [urljoin(url, a['href']) for a in soup.find_all('a', href=True)]\n",
    "            return response.text, links\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            return \"\", []\n",
    "\n",
    "    def filter_by_product_name(self, url_list):\n",
    "        return [url for url in url_list if self.product.lower() in url.lower()]\n",
    "\n",
    "    def scrape_links_to_dataframe(self, df, headers, blacklist):\n",
    "        session = requests.Session()\n",
    "        session.headers.update(headers)\n",
    "        df_links = {\"URL\": [], \"Links\": [], \"clean_url\": []}\n",
    "        for url in df['URL']:\n",
    "            content, extracted_links = self.extract_content_from_url(url, headers)\n",
    "            domain_name = self.extract_domain_from_url(url)\n",
    "            if domain_name not in blacklist:\n",
    "                clean_links = self.filter_by_product_name(extracted_links)\n",
    "                df_links[\"URL\"].append(url)\n",
    "                df_links[\"Links\"].append(extracted_links)\n",
    "                df_links[\"clean_url\"].append(clean_links)\n",
    "                time.sleep(15)\n",
    "        links_df = pd.DataFrame(df_links)\n",
    "        return links_df\n",
    "    \n",
    "    def extract_content_from_links(self, links_df, headers):\n",
    "        final_context = \"\"\n",
    "        for idx, row in links_df.iterrows():\n",
    "            if row['clean_url']:\n",
    "                for clean_url in row['clean_url']:\n",
    "                    content = self.extract_text_content_from_url(clean_url, headers)\n",
    "                    final_context += content + \" \"\n",
    "            else:\n",
    "                content = self.extract_text_content_from_url(row['URL'], headers)\n",
    "                final_context += content + \" \"\n",
    "        return final_context\n",
    "   \n",
    "    @staticmethod\n",
    "    def extract_text_content_from_url(url, headers):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            context = []\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.extract()\n",
    "            for tag in soup.find_all(True):\n",
    "                text = tag.get_text(strip=True)\n",
    "                if len(text.split()) > 100:\n",
    "                    context.append(text)\n",
    "            return ' '.join(context)\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_meaningful_sentences(text, keywords):\n",
    "        sentences = sent_tokenize(text)\n",
    "        meaningful_sentences = set()\n",
    "        for sentence in sentences:\n",
    "            for keyword in keywords:\n",
    "                if keyword in sentence.lower():\n",
    "                    cleaned_sentence = ' '.join(sentence.split())\n",
    "                    meaningful_sentences.add(cleaned_sentence)\n",
    "                    break\n",
    "        latest2 = ' '.join(meaningful_sentences)\n",
    "        return latest2\n",
    "\n",
    "    # Add the bert_extractive_summarize method here\n",
    "    def bert_extractive_summarize(self,text, terms, min_words=500, max_words=1000, chunk_size=500000):\n",
    "        # Initialize the BERT summarizer\n",
    "        model = Summarizer()\n",
    "\n",
    "        # Break the text into chunks\n",
    "        chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "        # Summarize each chunk\n",
    "        chunk_summaries = []\n",
    "        for chunk in chunks:\n",
    "            result = model(chunk)\n",
    "            chunk_summaries.append(\"\".join(result))\n",
    "\n",
    "        # Combine the summaries of all chunks\n",
    "        combined_summary = \" \".join(chunk_summaries)\n",
    "\n",
    "        # Filter sentences based on the terms\n",
    "        sentences = combined_summary.split('.')\n",
    "        relevant_sentences = [sent for sent in sentences if any(term.lower() in sent.lower() for term in terms)]\n",
    "\n",
    "        # If no relevant sentences are found, return the combined summary\n",
    "        if not relevant_sentences:\n",
    "            return combined_summary\n",
    "\n",
    "        # Calculate the word count of the summary\n",
    "        summary_words = combined_summary.split()\n",
    "\n",
    "        if len(summary_words) <= min_words:\n",
    "            return combined_summary\n",
    "        elif len(summary_words) >= max_words:\n",
    "            return ' '.join(relevant_sentences)\n",
    "\n",
    "        # If the word count is between min and max, return it\n",
    "        return combined_summary\n",
    "    \n",
    "\n",
    "    def get_completion(self, messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "        )\n",
    "        return response.choices[0].message[\"content\"]\n",
    "\n",
    "    def generate_analysis(self):\n",
    "        df = self.web_scraping()\n",
    "        links_df = self.scrape_links_to_dataframe(df, headers, blacklist)\n",
    "        final_context = self.extract_content_from_links(links_df, headers)\n",
    "        latest2 = self.extract_meaningful_sentences(final_context, keywords)\n",
    "        summary = self.bert_extractive_summarize(latest2, keywords, min_words=500, max_words=1000)\n",
    "        \n",
    "        # Start the conversation with the large text for pricing_concise\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": summary}\n",
    "        ]\n",
    "        question1 = f'''\n",
    "        In the context above, provide a short paragraph (max 100 words) on what is {self.product}, its price situation, and the latest reasons for it (excluding any mentions of covid, corona, or covid-19 and price figures).. Dont include any reference report or company name.\n",
    "        '''\n",
    "        messages.append({\"role\": \"user\", \"content\": question1})\n",
    "        pricing_concise = self.get_completion(messages)\n",
    "        \n",
    "        # Reset messages for market_analysis\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": summary}\n",
    "        ]\n",
    "        question2 = f'''\n",
    "        In the context above, explain the market situation, supply, and demand of {self.product} in {self.country} in three separate short paragraphs with heading as Market Situation, Supply, and Demand. Dont include any reference report or company name.\n",
    "        '''\n",
    "        messages.append({\"role\": \"user\", \"content\": question2})\n",
    "        market_analysis = self.get_completion(messages)\n",
    "        \n",
    "        return market_analysis, pricing_concise\n",
    "\n",
    "#obj = MarketAnalysisGenerator('Amonia', 'USA', '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')\n",
    "#market_analysis, pricing_concise = obj.generate_analysis()\n",
    "#sections = market_analysis.split('\\n\\n')\n",
    "#paragraphs = [section.split(':', 1)[1].strip() for section in sections]\n",
    "#market_situation=paragraphs[0]\n",
    "#supply=paragraphs[1]\n",
    "#demand=paragraphs[2]\n",
    "#print(pricing_concise)\n",
    "\n",
    "class other_than_first_week:\n",
    "    \n",
    "    def __init__(self, product, country, openai_api_key):\n",
    "        self.product = product\n",
    "        self.country = country\n",
    "        self.openai_api_key = openai_api_key\n",
    "        openai.api_key = self.openai_api_key\n",
    "\n",
    "    @staticmethod\n",
    "    def past_dates(days):\n",
    "        current_date = datetime.now()\n",
    "        past_date = current_date - timedelta(days=days) \n",
    "        return current_date.strftime(\"%m/%d/%Y\"), past_date.strftime(\"%m/%d/%Y\")\n",
    "\n",
    "    def web_scraping(self, days=7):\n",
    "        current_date, past_seven = self.past_dates(days)\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        text = f'{self.product} market situation {self.country}'\n",
    "        start_date = past_seven\n",
    "        end_date = current_date\n",
    "        date_range = f\"&tbs=cdr:1,cd_min:{urllib.parse.quote_plus(start_date)},cd_max:{urllib.parse.quote_plus(end_date)}\"\n",
    "        url = f'https://google.com/search?q={urllib.parse.quote_plus(text)}{date_range}'\n",
    "        \n",
    "        time.sleep(5)\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        search_results = soup.select('div.tF2Cxc')\n",
    "        data = []\n",
    "        fetched_urls_count = 0\n",
    "        for result in search_results:\n",
    "            title = result.select_one('h3').text if result.select_one('h3') else None\n",
    "            link = result.select_one('a')['href'] if result.select_one('a') else None\n",
    "            source_name = result.select_one('div.TbwUpd.NJjxre').text if result.select_one('div.TbwUpd.NJjxre') else None\n",
    "            if not any(blacklisted_item in link for blacklisted_item in blacklist):\n",
    "                try:\n",
    "                    \n",
    "                    source_response = requests.get(link, headers=headers)\n",
    "                    source_soup = BeautifulSoup(source_response.content, 'html.parser')\n",
    "                    source_text = source_soup.get_text()\n",
    "                    doc = nlp(source_text)\n",
    "                    relevant_answers = [sentence.text for sentence in doc.sents if text.lower() in sentence.text.lower()]\n",
    "                    data.append({\n",
    "                        \"Title\": title,\n",
    "                        \"URL\": link,\n",
    "                        \"Source Name\": source_name,\n",
    "                        \"Answer\": \"\\n\".join(relevant_answers)\n",
    "                    })\n",
    "                    fetched_urls_count += 1\n",
    "                    if fetched_urls_count >= 10:\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error fetching content from {link}: {str(e)}\")\n",
    "        df = pd.DataFrame(data, columns=[\"Title\", \"URL\", \"Source Name\", \"Answer\"])\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_domain_from_url(url):\n",
    "        parsed_uri = urlparse(url)\n",
    "        domain = '{uri.netloc}'.format(uri=parsed_uri).replace(\"www.\", \"\")\n",
    "        return domain\n",
    "\n",
    "    @staticmethod\n",
    "    def is_blacklisted(content, blacklist):\n",
    "        return any(keyword in content.lower() for keyword in blacklist)\n",
    "\n",
    "    def extract_content_from_url(self, url, headers):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            links = [urljoin(url, a['href']) for a in soup.find_all('a', href=True)]\n",
    "            return response.text, links\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            return \"\", []\n",
    "\n",
    "    def filter_by_product_name(self, url_list):\n",
    "        return [url for url in url_list if self.product.lower() in url.lower()]\n",
    "\n",
    "    def scrape_links_to_dataframe(self, df, headers, blacklist):\n",
    "        session = requests.Session()\n",
    "        session.headers.update(headers)\n",
    "        df_links = {\"URL\": [], \"Links\": [], \"clean_url\": []}\n",
    "        for url in df['URL']:\n",
    "            content, extracted_links = self.extract_content_from_url(url, headers)\n",
    "            domain_name = self.extract_domain_from_url(url)\n",
    "            if domain_name not in blacklist:\n",
    "                clean_links = self.filter_by_product_name(extracted_links)\n",
    "                df_links[\"URL\"].append(url)\n",
    "                df_links[\"Links\"].append(extracted_links)\n",
    "                df_links[\"clean_url\"].append(clean_links)\n",
    "                time.sleep(15)\n",
    "        links_df = pd.DataFrame(df_links)\n",
    "        return links_df\n",
    "    \n",
    "    def extract_content_from_links(self, links_df, headers):\n",
    "        final_context = \"\"\n",
    "        for idx, row in links_df.iterrows():\n",
    "            if row['clean_url']:\n",
    "                for clean_url in row['clean_url']:\n",
    "                    content = self.extract_text_content_from_url(clean_url, headers)\n",
    "                    final_context += content + \" \"\n",
    "            else:\n",
    "                content = self.extract_text_content_from_url(row['URL'], headers)\n",
    "                final_context += content + \" \"\n",
    "        return final_context\n",
    "   \n",
    "    @staticmethod\n",
    "    def extract_text_content_from_url(url, headers):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            context = []\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.extract()\n",
    "            for tag in soup.find_all(True):\n",
    "                text = tag.get_text(strip=True)\n",
    "                if len(text.split()) > 100:\n",
    "                    context.append(text)\n",
    "            return ' '.join(context)\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_meaningful_sentences(text, keywords):\n",
    "        sentences = sent_tokenize(text)\n",
    "        meaningful_sentences = set()\n",
    "        for sentence in sentences:\n",
    "            for keyword in keywords:\n",
    "                if keyword in sentence.lower():\n",
    "                    cleaned_sentence = ' '.join(sentence.split())\n",
    "                    meaningful_sentences.add(cleaned_sentence)\n",
    "                    break\n",
    "        latest2 = ' '.join(meaningful_sentences)\n",
    "        return latest2\n",
    "\n",
    "    # Add the bert_extractive_summarize method here\n",
    "    def bert_extractive_summarize(self,text, terms, min_words=500, max_words=1000, chunk_size=500000):\n",
    "        # Initialize the BERT summarizer\n",
    "        model = Summarizer()\n",
    "\n",
    "        # Break the text into chunks\n",
    "        chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "        # Summarize each chunk\n",
    "        chunk_summaries = []\n",
    "        for chunk in chunks:\n",
    "            result = model(chunk)\n",
    "            chunk_summaries.append(\"\".join(result))\n",
    "\n",
    "        # Combine the summaries of all chunks\n",
    "        combined_summary = \" \".join(chunk_summaries)\n",
    "\n",
    "        # Filter sentences based on the terms\n",
    "        sentences = combined_summary.split('.')\n",
    "        relevant_sentences = [sent for sent in sentences if any(term.lower() in sent.lower() for term in terms)]\n",
    "\n",
    "        # If no relevant sentences are found, return the combined summary\n",
    "        if not relevant_sentences:\n",
    "            return combined_summary\n",
    "\n",
    "        # Calculate the word count of the summary\n",
    "        summary_words = combined_summary.split()\n",
    "\n",
    "        if len(summary_words) <= min_words:\n",
    "            return combined_summary\n",
    "        elif len(summary_words) >= max_words:\n",
    "            return ' '.join(relevant_sentences)\n",
    "\n",
    "        # If the word count is between min and max, return it\n",
    "        return combined_summary\n",
    "    \n",
    "\n",
    "    def get_completion(self, messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "        )\n",
    "        return response.choices[0].message[\"content\"]\n",
    "\n",
    "    def generate_analysis(self):\n",
    "        df = self.web_scraping()\n",
    "        links_df = self.scrape_links_to_dataframe(df, headers, blacklist)\n",
    "        final_context = self.extract_content_from_links(links_df, headers)\n",
    "        latest2 = self.extract_meaningful_sentences(final_context, keywords)\n",
    "        summary = self.bert_extractive_summarize(latest2, keywords, min_words=500, max_words=1000)\n",
    "        \n",
    "        # Start the conversation with the large text for pricing_concise\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": summary}\n",
    "        ]\n",
    "        question1 = f'''\n",
    "        In the context above, provide a summary in  paragraph (atleast 300 and max 350 words) on what is {self.product}, its price and market situation with supply and demand (excluding any mentions of covid, corona, or covid-19 and price figures).. Dont include any reference report or company name.\n",
    "        '''\n",
    "        messages.append({\"role\": \"user\", \"content\": question1})\n",
    "        pricing_concise = self.get_completion(messages)\n",
    "        \n",
    "        \n",
    "        return pricing_concise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab12531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.message import EmailMessage\n",
    "\n",
    "def send_email(subject, body, to_email):\n",
    "    # Your email details\n",
    "    EMAIL_ADDRESS = 'your_email@gmail.com'\n",
    "    EMAIL_PASSWORD = 'your_password'\n",
    "    \n",
    "    # Setting up the email content\n",
    "    msg = EmailMessage()\n",
    "    msg.set_content(body)\n",
    "    msg['Subject'] = subject\n",
    "    msg['From'] = EMAIL_ADDRESS\n",
    "    msg['To'] = to_email\n",
    "    \n",
    "    # Using Gmail's SMTP server to send the email\n",
    "    with smtplib.SMTP_SSL('smtp.gmail.com', 465) as smtp:\n",
    "        smtp.login(EMAIL_ADDRESS, EMAIL_PASSWORD)\n",
    "        smtp.send_message(msg)\n",
    "def your_function():\n",
    "    try:\n",
    "        # Your original code here\n",
    "        pass\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        send_email('Error in your code', f'Something went wrong: {error_msg}', 'member_email@example.com')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c352ceec",
   "metadata": {},
   "source": [
    "# Version1.0 code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f05e677",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18834110",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\web.dev-1\\AppData\\Local\\Temp\\ipykernel_6212\\3421183680.py:30: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  trigger = pd.read_sql(trigger_query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching https://www.digitaljournal.com/pr/news/prwirecenter/bio-polyamide-market-share-growth-2023-with-usd-321-17-million-and-cagr-value-13-09-by-2028-126-pages-report: 403 Client Error: Forbidden for url: https://www.digitaljournal.com/pr/news/prwirecenter/bio-polyamide-market-share-growth-2023-with-usd-321-17-million-and-cagr-value-13-09-by-2028-126-pages-report\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import logging\n",
    "import calendar\n",
    "# Configure logging\n",
    "log_file_path = 'C:/Users/web.dev-1/Downloads/v1_logs.txt'\n",
    "logging.basicConfig(filename=log_file_path, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "import pandas as pd\n",
    "import time\n",
    "from statistics import mean, median, stdev, variance\n",
    "import scipy.stats as stats\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import json\n",
    "import pyodbc\n",
    "\n",
    "server = 'TECH-98'\n",
    "database = 'chem_prod_copy'\n",
    "username = 'sa'\n",
    "password = '123456'\n",
    "\n",
    "connection_string = f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}\"\n",
    "\n",
    "conn = pyodbc.connect(connection_string)\n",
    "cursor = conn.cursor()\n",
    "logging.info(cursor)\n",
    "\n",
    "\n",
    "# Function to call the API (dummy function for illustration)\n",
    "def call_api_and_generate_summary(product, country,current_log):\n",
    "    \n",
    "    trigger_query='Select * from tracking'\n",
    "    trigger = pd.read_sql(trigger_query, conn)\n",
    "    curr_log_id=trigger.loc[trigger['LogID']==current_log]\n",
    "    date_added=curr_log_id['InsertedDateTime'].iloc[0]\n",
    "    date_added = pd.Timestamp(date_added)\n",
    "    current_date = datetime.now()\n",
    "    is_in_first_week = date_added.month == current_date.month and date_added.day <= 7\n",
    "\n",
    "    if is_in_first_week:    \n",
    "\n",
    "        # Extract the month name\n",
    "        month = date_added.strftime(\"%B\")\n",
    "        previous_month = date_added.month - 1 if date_added.month > 1 else 12\n",
    "        previous_month_name = calendar.month_name[previous_month]\n",
    "\n",
    "        query1 = 'SELECT * FROM SA_ChemPriceWeeklyNew'\n",
    "        week = pd.read_sql(query1, conn)\n",
    "        query2 = ' SELECT * FROM SA_CommentaryPricing'\n",
    "        market = pd.read_sql(query2, conn)\n",
    "\n",
    "        query3='Select * from SA_MaintenanceAndShutdown'\n",
    "\n",
    "\n",
    "        track=pd.read_sql(query3,conn)\n",
    "        # Your API call and summary generation logic here\n",
    "        # week=pd.read_excel('weekly_data.xlsx')\n",
    "        # week=week.loc[week['Range']=='Weekly']\n",
    "        week1 = week.loc[(week['Product'] ==product) & (week['Country']==country)]\n",
    "        week1 = week1.rename({'ProductVariant': 'ProductVarient'}, axis=1)\n",
    "        market1 = market.loc[market['Product'] == product]\n",
    "        product_df = pd.merge(market1, week1, on=['ProductVarient'], how='inner')\n",
    "        product_df['Date'] = pd.to_datetime(product_df['Date'], errors='coerce')\n",
    "        newest = product_df.sort_values('Date', ascending=False)\n",
    "        product_df = newest.loc[newest['year_x'] == '2023']\n",
    "        # product_df = product_df.sort_values(by='Date', ascending=False)\n",
    "\n",
    "        product_name = product_df['ProductVarient'].iloc[0]\n",
    "        # Calculate statistical values\n",
    "        average_price = int(mean(product_df['count']))\n",
    "        median_price = int(median(product_df['count']))\n",
    "        highest_price = int(max(product_df['count']))\n",
    "        lowest_price = int(min(product_df['count']))\n",
    "        std_dev = int(stdev(product_df['count']))\n",
    "        var = int(variance(product_df['count']))\n",
    "        skewness = int(stats.skew(product_df['count']))\n",
    "        kurtosis = int(stats.kurtosis(product_df['count']))\n",
    "\n",
    "        # product_df['Moving_Avg'] = product_df['count'].rolling(window=7).mean()\n",
    "\n",
    "        # Time series decomposition for trend and seasonality\n",
    "        # Assuming df is indexed by date\n",
    "        decomposed = seasonal_decompose(product_df['count'], period=7)\n",
    "        trend_percentage = int(\n",
    "            (decomposed.trend.dropna().iloc[-1] - decomposed.trend.dropna().iloc[0]) / decomposed.trend.dropna().iloc[\n",
    "                0] * 100)\n",
    "        seasonality_percentage = int(decomposed.seasonal.dropna().max() * 100)\n",
    "\n",
    "        # Get the latest market situation, demand, supply, and plant shutdown info\n",
    "        latest_market_situation = product_df['MarketSituation'].iloc[0]\n",
    "        latest_demand = product_df['Demand'].iloc[0]\n",
    "        latest_supply = product_df['Supply'].iloc[0]\n",
    "\n",
    "\n",
    "        # week_number=week_number_to_month_and_week(product_df['Week'].iloc[0])[1]\n",
    "        # month = week_number_to_month_and_week(product_df['Week'].iloc[0])[0]\n",
    "        newest['month'] = newest['Date'].dt.month_name(locale='English')\n",
    "        # newest['month'] = newest['Date'].dt.month\n",
    "        #month = newest['month'].iloc[0]\n",
    "        # Generate the HTML summary using f-strings\n",
    "\n",
    "        df = product_df[['Date', 'count', 'Min', 'Max']].drop_duplicates()\n",
    "        df['Date'] = pd.to_datetime(df['Date'], format='%d-%B-%Y')\n",
    "\n",
    "        # Sort the DataFrame by date in descending order\n",
    "        df = df.sort_values(by='Date', ascending=False)\n",
    "\n",
    "        # Find the latest maximum price and its corresponding date\n",
    "        latest_max = df.iloc[0]\n",
    "\n",
    "        # Find the index of the latest maximum price in the sorted DataFrame\n",
    "        latest_max_index = df.index[0]\n",
    "\n",
    "        # Find the previous occurrences of this maximum price in different months\n",
    "        previous_max_occurrences = df[(df['Max'] == latest_max['Max']) & (df['Date'].dt.month != latest_max['Date'].month)]\n",
    "\n",
    "        # Calculate the time difference in months\n",
    "        latest_date = latest_max['Date']\n",
    "        previous_dates = previous_max_occurrences['Date']\n",
    "        months_difference = (latest_date - previous_dates).dt.days // 30\n",
    "\n",
    "        if ((track['Product'] == product) & (track['Country'] == country)).any() == True:\n",
    "            dd = track.loc[(track['Product'] == product) & (track['Country'] == country)]\n",
    "            dd['CreatedDate'] = pd.to_datetime(dd['CreatedDate'], errors='coerce')\n",
    "            dd2 = dd.sort_values(by='CreatedDate', ascending=False)\n",
    "            dd2=dd2.loc[dd2['CreatedDate']>'2023-01-01']\n",
    "            dd2 = dd2.loc[dd2['Month'] == month]\n",
    "\n",
    "\n",
    "            # Initialize the maintenance summary string outside the loop to prevent it from being reset\n",
    "            maintenance_summary = \"\"\n",
    "\n",
    "            # Check if there's any maintenance data\n",
    "\n",
    "            keywords = ['maintenance', 'force', 'permanent']\n",
    "            maintenance_data = dd2[dd2['TypeOfShutdown'].str.contains('|'.join(keywords), case=False, na=False)]\n",
    "\n",
    "            for _, row in maintenance_data.iterrows():\n",
    "                maintenance_summary += f'''\n",
    "                {row['Company']} in {row['Location']}, {row['Country']}, experienced a {row['Duration']} days {row['TypeOfShutdown']} from {row['OutageStartDate']} to {row['OutageEndDate']}, due to {row['Insights']} in the {row['Region']} region, resulting in a {row['CapacityLoss']}KT capacity loss.\n",
    "                '''\n",
    "\n",
    "            # If no maintenance data, set a default message\n",
    "            if not maintenance_summary:\n",
    "                maintenance_summary = \"No maintenance shutdown detected for the selected month and product.\"\n",
    "\n",
    "        else:\n",
    "\n",
    "            maintenance_summary = product_df['PlantShutdown'].iloc[0]\n",
    "\n",
    "\n",
    "        ###################################### now call the version 2 ###########################################\n",
    "        try:\n",
    "\n",
    "\n",
    "            product_name = curr_log_id['type_week'].iloc[0]\n",
    "            week_avg_price=curr_log_id['count'].iloc[0]\n",
    "            week_min=curr_log_id['Min'].iloc[0]\n",
    "            week_max=curr_log_id['Max'].iloc[0]\n",
    "            current_week_string=f'''\n",
    "            The average price of {product_name} in {country} for the current week was ${week_avg_price}, with the highest price reaching ${week_max} and the lowest price dropping to ${week_min} during the week.'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            obj = MarketAnalysisGenerator(product_name, country, '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')\n",
    "            market_analysis, pricing_concise = obj.generate_analysis()\n",
    "            paragraphs = market_analysis.split('\\n\\n')\n",
    "            pricing_concise = pricing_concise +''+ current_week_string\n",
    "            print(pricing_concise)\n",
    "\n",
    "\n",
    "            # Generate the HTML summary incorporating the maintenance summary\n",
    "\n",
    "            html_summary = f\"\"\"<!DOCTYPE html>\n",
    "                    <html>\n",
    "                    <head>\n",
    "                        <title>Product Price Analysis and Summary</title>\n",
    "                    </head>\n",
    "                    <body>\n",
    "\n",
    "                    <h1>Product Price Analysis and Summary</h1>\n",
    "                    <h2>Price Summary:</h2>\n",
    "                    <p>{pricing_concise}</p>\n",
    "                    <h2>Price Overview: Last Month:</h2>\n",
    "\n",
    "                    <ul>\n",
    "                        <li>Average Price: The average price of {product_name} during the month of {previous_month_name} was approximately USD {average_price}</li>\n",
    "                        <li>Median Price: The median price , representing the middle value of {product_name} during {previous_month_name}, was USD {median_price}.</li>\n",
    "                        <li>Highest Price: {product_name} reached its highest price at USD {highest_price} during the month of {previous_month_name}. It took approximately {months_difference.min()} months to reach this peak price again.</li>\n",
    "                        <li>Lowest Price: {product_name} recorded its lowest price at USD {lowest_price} during the month of {previous_month_name}. It took approximately 4 months to reach this low price again.</li>\n",
    "                        <li>Trend Percentage: Price trend during the month of {previous_month_name} was {trend_percentage}%</li>\n",
    "                        <li>Seasonality Percentage: Price Seasonality during the month of {previous_month_name} was {seasonality_percentage}%</li>\n",
    "\n",
    "                    </ul>\n",
    "\n",
    "                    <h2>Statistical Insights:</h2>\n",
    "                    <ul>\n",
    "                        <li>Standard Deviation: The standard deviation in the price was USD {std_dev}. A higher standard deviation suggests greater price volatility, which could impact strategic planning and risk management.</li>\n",
    "                        <li>Skewness: {skewness}, indicating a symmetric price distribution. A positive or negative skewness might suggest a bias in price trends that requires attention.</li>\n",
    "                        <li>Kurtosis: The kurtosis value was {kurtosis}, indicating a normally distributed price. A higher kurtosis could imply more extreme price fluctuations, influencing long-term pricing strategies.</li>\n",
    "                    </ul>\n",
    "\n",
    "\n",
    "                    <h2>Market Situation:</h2>\n",
    "                    {''.join(f'<p>{p}</p>' for p in paragraphs)}\n",
    "\n",
    "                    <h2>Plant Shutdowns:</h2>\n",
    "                    <ul>\n",
    "                        <li>{maintenance_summary}</li>\n",
    "                    </ul>\n",
    "                    </body>\n",
    "                    </html>\"\"\"\n",
    "\n",
    "            return html_summary\n",
    "        except:\n",
    "            error='something went wrong in the code'\n",
    "            return error\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        product_name = curr_log_id['type_week'].iloc[0]\n",
    "        week_avg_price=curr_log_id['count'].iloc[0]\n",
    "        week_min=curr_log_id['Min'].iloc[0]\n",
    "        week_max=curr_log_id['Max'].iloc[0]\n",
    "        current_week_string=f'''\n",
    "        The average price of {product_name} in {country} for the current week was ${week_avg_price}, with the highest price reaching ${week_max} and the lowest price dropping to ${week_min} during the week.'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        obj = MarketAnalysisGenerator(product_name, country, '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')\n",
    "        market_analysis, pricing_concise = obj.generate_analysis()\n",
    "        paragraphs = market_analysis.split('\\n\\n')\n",
    "        pricing_concise = pricing_concise +''+ current_week_string\n",
    "        \n",
    "        html_summary = f\"\"\"<!DOCTYPE html>\n",
    "                    <html>\n",
    "                    <head>\n",
    "                        <title>Product Price Analysis and Summary</title>\n",
    "                    </head>\n",
    "                    <body>\n",
    "                    <h2>Short Summary</h2>\n",
    "                    <p> {pricing_concise} </p>\n",
    "                    </body>\n",
    "                    </html>\"\"\"\n",
    "\n",
    "        return html_summary\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "last_processed_log_id = 0\n",
    "df_logs = pd.DataFrame(columns=['LogID', 'Product', 'InsertedDateTime', 'Country', 'Status'])\n",
    "\n",
    "while True:\n",
    "    new_logs_list = []\n",
    "\n",
    "    query = f\"SELECT * FROM tracking WHERE LogID > {last_processed_log_id} ORDER BY LogID\"\n",
    "    cursor.execute(query)\n",
    "    logs = cursor.fetchall()\n",
    "\n",
    "    for log in logs:\n",
    "        # Check if this LogID is already processed and marked as 'Done' in the database\n",
    "        cursor.execute(f\"SELECT Status FROM tracking WHERE LogID = {log[0]}\")\n",
    "        db_status = cursor.fetchone()[0]\n",
    "\n",
    "        if db_status == 'Done':\n",
    "            logging.info(f\"Skipping LogID {log[0]} as it is already done.\")\n",
    "            continue\n",
    "\n",
    "        # Process the log data here (print, store, or any other operation)\n",
    "        logging.info(log)\n",
    "        last_processed_log_id = log[0]  # Assuming LogID is the first column\n",
    "\n",
    "        # Call API and generate summary\n",
    "        logging.info(log[1])\n",
    "        summary = call_api_and_generate_summary(log[1], log[10],log[0])\n",
    "\n",
    "        logging.info(summary)\n",
    "        if summary:\n",
    "            # Convert the summary dictionary to a JSON string\n",
    "            summary_json = json.dumps(summary)\n",
    "\n",
    "            cursor.execute(f\"UPDATE tracking SET Status = 'Done', Summary = ? WHERE LogID = ?\", (summary_json, log[0]))\n",
    "            # Commit the transaction to save changes\n",
    "            conn.commit()\n",
    "        else:\n",
    "            logging.info(f\"Summary is None for LogID {log[0]}\")\n",
    "\n",
    "        # Update the status in the database\n",
    "        if summary:\n",
    "            cursor.execute(f\"UPDATE tracking SET Status = 'Done' WHERE LogID = {log[0]}\")\n",
    "            # Commit the transaction to save changes\n",
    "            conn.commit()\n",
    "\n",
    "        log_dict = {\n",
    "            'LogID': log[0],\n",
    "            'Product': log[1],\n",
    "            # 'ProductVariant': log[2],\n",
    "            'InsertedDateTime': log[-1],\n",
    "            'Country': log[10],\n",
    "            'Status': 'Done' if summary else 'Pending'\n",
    "        }\n",
    "        new_logs_list.append(log_dict)\n",
    "\n",
    "    # Append new logs to the DataFrame only if there are new logs\n",
    "    if new_logs_list:\n",
    "        df_new_logs = pd.DataFrame(new_logs_list)\n",
    "        df_logs = pd.concat([df_logs, df_new_logs], ignore_index=True)\n",
    "\n",
    "    # Sleep for a defined interval (e.g., 10 seconds) before checking again\n",
    "    time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea87ab9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158582f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482ee79b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c578793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd9abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04debddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975e7373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88485cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4c4619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f355662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2776dbd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a30e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7d9d080",
   "metadata": {},
   "source": [
    "# Optional scraping from google news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9f123ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>URL</th>\n",
       "      <th>Publish Date</th>\n",
       "      <th>Description</th>\n",
       "      <th>Page Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Caprolactam Market: 2023 to 2027 Analysis of M...</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMiU2h0d...</td>\n",
       "      <td>2023-02-06 08:00:00</td>\n",
       "      <td>&lt;a href=\"https://news.google.com/rss/articles/...</td>\n",
       "      <td>&lt;!doctype html&gt;&lt;html lang=\"en-IN\" dir=\"ltr\"&gt;&lt;h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Caprolactam Global Market Report 2022: Growing...</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMioAFod...</td>\n",
       "      <td>2023-01-05 08:00:00</td>\n",
       "      <td>&lt;a href=\"https://news.google.com/rss/articles/...</td>\n",
       "      <td>&lt;!doctype html&gt;&lt;html lang=\"en-IN\" dir=\"ltr\"&gt;&lt;h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "7   Caprolactam Market: 2023 to 2027 Analysis of M...   \n",
       "28  Caprolactam Global Market Report 2022: Growing...   \n",
       "\n",
       "                                                  URL        Publish Date  \\\n",
       "7   https://news.google.com/rss/articles/CBMiU2h0d... 2023-02-06 08:00:00   \n",
       "28  https://news.google.com/rss/articles/CBMioAFod... 2023-01-05 08:00:00   \n",
       "\n",
       "                                          Description  \\\n",
       "7   <a href=\"https://news.google.com/rss/articles/...   \n",
       "28  <a href=\"https://news.google.com/rss/articles/...   \n",
       "\n",
       "                                         Page Content  \n",
       "7   <!doctype html><html lang=\"en-IN\" dir=\"ltr\"><h...  \n",
       "28  <!doctype html><html lang=\"en-IN\" dir=\"ltr\"><h...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import urlencode\n",
    "import requests\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "\n",
    "# Set the headers with the User-Agent\n",
    "headers = {\n",
    "    \"User-Agent\": user_agent\n",
    "}\n",
    "\n",
    "# Specify the base URL for Google News RSS\n",
    "base_url = \"https://news.google.com/news/rss\"\n",
    "\n",
    "# Define your query parameters\n",
    "products = [\"caprolactam\"]  # Replace with your list of products\n",
    "country = \"china\"  # Replace with the desired country code\n",
    "\n",
    "# Construct the complete URL with query and country using f-strings\n",
    "query = f\"why {', '.join(products)} market is going up in {country} ?\"\n",
    "params = {\n",
    "    \"q\": query,\n",
    "    \"hl\": country\n",
    "}\n",
    "query_url = base_url + \"?\" + urlencode(params)\n",
    "\n",
    "# Open the URL and read the XML\n",
    "Client = urlopen(query_url)\n",
    "xml_page = Client.read()\n",
    "Client.close()\n",
    "\n",
    "soup_page = soup(xml_page, \"xml\")\n",
    "news_list = soup_page.findAll(\"item\")\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "data = []\n",
    "for news in news_list:\n",
    "    title = news.title.text\n",
    "    link = news.link.text\n",
    "    pub_date = news.pubDate.text\n",
    "\n",
    "    # Get the content of the page\n",
    "    try:\n",
    "        response = requests.get(link, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        page_content = response.text\n",
    "\n",
    "        # Extract short description if available\n",
    "        description = news.description.text if news.description else \"No description available\"\n",
    "\n",
    "        data.append([title, link, pub_date, description, page_content])\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching content from {link}: {e}\")\n",
    "\n",
    "# Define column names for the DataFrame\n",
    "columns = [\"Title\", \"URL\", \"Publish Date\", \"Description\", \"Page Content\"]\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "filtered_df = pd.DataFrame()\n",
    "for product in products:\n",
    "    filtered_df = pd.concat([filtered_df, df[df['Title'].str.contains(product, case=False, regex=True)]])\n",
    "\n",
    "# Sort the filtered DataFrame by the \"Publish Date\" column in descending order\n",
    "filtered_df[\"Publish Date\"] = pd.to_datetime(filtered_df[\"Publish Date\"])\n",
    "filtered_df = filtered_df.sort_values(by=\"Publish Date\", ascending=False)\n",
    "\n",
    "filtered_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95f8c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b72c247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77913325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5649979b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5dcd90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83f81ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3d0fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574b5ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbcbe46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f90b86f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918903d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50eff38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
